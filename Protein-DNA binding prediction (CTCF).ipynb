{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Protein-DNA binding prediction (CTCF)\n",
    "=============\n",
    "\n",
    "Goal: implement DeepBing with TensorFlow\n",
    "------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "***\n",
    "## DeepBind:\n",
    "\n",
    "> <img src=\"http://www.nature.com/nbt/journal/v33/n8/images/nbt.3300-F2.jpg\" width=\"80%\">\n",
    "> Source: http://www.nature.com/nbt/journal/v33/n8/images/nbt.3300-F2.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train.pickle:  (77531, 121, 4) (77531, 2)\n",
      "Loaded test.pickle:  (19383, 121, 4)\n",
      "\n",
      "Training set:\t range(0, 62024) (62024, 121, 4) (62024, 2)\n",
      "Validation set:\t range(62025, 77531) (15506, 121, 4) (15506, 2)\n",
      "Test set:\t range(0, 19383) (19383, 121, 4) (19383, 2)\n"
     ]
    }
   ],
   "source": [
    "K_FOLD = [4,1] ## A 5 fold\n",
    "TRAIN_PICKLE = '../../../train.pickle'\n",
    "TEST_PICKLE  = '../../../test.pickle'\n",
    "\n",
    "with open(TRAIN_PICKLE, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    seq  = save['seq']\n",
    "    label= save['label']\n",
    "    len1 = seq.shape[0]\n",
    "    del save\n",
    "    print('Loaded train.pickle: ',seq.shape,label.shape)\n",
    "\n",
    "with open(TEST_PICKLE, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    test_seq = save['seq']\n",
    "    test_label = save['label']\n",
    "    len2 = test_seq.shape[0]\n",
    "    del save\n",
    "    print('Loaded test.pickle: ',test_seq.shape)\n",
    "\n",
    "\n",
    "_ = int((len1-len1%(sum(K_FOLD)))/(sum(K_FOLD))) ## 15506\n",
    "r_train = range(0,4*_)\n",
    "r_valid = range(4*_+1,len1)\n",
    "r_test  = range(0,len2)\n",
    "\n",
    "train_dataset, train_labels = seq[r_train,], label[r_train,]  \n",
    "valid_dataset, valid_labels = seq[r_valid,], label[r_valid,]\n",
    "test_dataset, test_labels   = seq[r_test,] , label[r_test,]  \n",
    "\n",
    "print('\\nTraining set:\\t',     r_train,  train_dataset.shape,  train_labels.shape)\n",
    "print('Validation set:\\t', r_valid,  valid_dataset.shape,  valid_labels.shape)\n",
    "print('Test set:\\t',         r_test,   test_dataset.shape,   test_labels.shape)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (62024, 121, 4, 1) (62024, 2)\n",
      "Validation set (15506, 121, 4, 1) (15506, 2)\n",
      "Test set (19383, 121, 4, 1) (19383, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_channels = 1\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, 121, 4, num_channels)).astype(np.float32)\n",
    "#   labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepBind CNN Model\n",
    "> <img src=\"http://www.nature.com/nbt/journal/v33/n8/images/nbt.3300-SF1.jpg\" width=\"90%\">\n",
    ">... Shown is an example with __batch_size=5, motif_len=6, num_motifs=4, num_models=3__. Sequences are padded with ‘N’s so that the motif scan operation can find detections at both extremities. Yellow cells represent the reverse complement of the input located above; both strands are fed to the model, and the strand with the maximum score is used for the output prediction (the max strand stage). The output dimension of the pool stage, depicted as num_motifs (*), depends on whether “max” or “max and avg” pooling was used.\n",
    "> \n",
    "> Image source: http://www.nature.com/nbt/journal/v33/n8/fig_tab/nbt.3300_SF1.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           [batch, height, width, channel]\n",
      "data:      [256, 121, 4, 1]\n",
      "conv:      [256, 121, 4, 16]\n",
      "relu:      [256, 121, 4, 16]\n",
      "pooling:   [256, 60, 2, 16]\n",
      "reshape:   [256, 1920]\n",
      "hidden:    [256, 32]\n",
      "output:    [256, 2] \n",
      "\n",
      "\n",
      "           [batch, height, width, channel]\n",
      "data:      [15506, 121, 4, 1]\n",
      "conv:      [15506, 121, 4, 16]\n",
      "relu:      [15506, 121, 4, 16]\n",
      "pooling:   [15506, 60, 2, 16]\n",
      "reshape:   [15506, 1920]\n",
      "hidden:    [15506, 32]\n",
      "output:    [15506, 2] \n",
      "\n",
      "\n",
      "           [batch, height, width, channel]\n",
      "data:      [19383, 121, 4, 1]\n",
      "conv:      [19383, 121, 4, 16]\n",
      "relu:      [19383, 121, 4, 16]\n",
      "pooling:   [19383, 60, 2, 16]\n",
      "reshape:   [19383, 1920]\n",
      "hidden:    [19383, 32]\n",
      "output:    [19383, 2] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_size   = [121,4]  ## 101bps, plus 10bps frenking on both end\n",
    "num_labels   = 2        ## bind or not (1 or 0)\n",
    "batch_size   = 256      ## TODO: try with double strand input!\n",
    "filter_size  = [11,4]   ## Motif detector length = 11 (about 1.5 times of expected motif length)\n",
    "depth        = 16       ## Number of motif detector (num_motif) = 16\n",
    "num_hidden   = 32       ## 32 ReLU units of no hidden layer at all\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size[0], image_size[1], num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # conv filter, shape=(11,3)\n",
    "    filter_W = tf.Variable(tf.truncated_normal([11, 3, num_channels, depth], stddev=0.1)) ## difference ?\n",
    "    filter_b = tf.Variable(tf.zeros([depth]))\n",
    "\n",
    "    # NN layer, shape=()\n",
    "    hidden_W = tf.Variable(tf.truncated_normal([1920, 32], stddev=0.1))\n",
    "    hidden_b = tf.Variable(tf.constant(1.0, shape=[32]))\n",
    "    \n",
    "    # output layer, shape=()\n",
    "    output_W = tf.Variable(tf.truncated_normal([32, 2], stddev=0.1))\n",
    "    output_b = tf.Variable(tf.constant(1.0, shape=[2]))\n",
    " \n",
    "\n",
    "    ## Model.\n",
    "    def model(data):\n",
    "        print('           [batch, height, width, channel]') \n",
    "        print('data:     ', data.get_shape().as_list())\n",
    "        \n",
    "        # Convolution: (121,4,1) ---- 3x6 filter ---> (121,4,4)\n",
    "        conv = tf.nn.conv2d(data, filter_W, [1, 1, 1, 1], padding='SAME')\n",
    "        print('conv:     ', conv.get_shape().as_list())\n",
    "        \n",
    "        relu = tf.nn.relu(conv + filter_b)\n",
    "        print('relu:     ', relu.get_shape().as_list())\n",
    "\n",
    "        pool = tf.nn.max_pool(relu, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "        print('pooling:  ', pool.get_shape().as_list())\n",
    "        \n",
    "        shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        print('reshape:  ', reshape.get_shape().as_list())\n",
    "        \n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, hidden_W) + hidden_b)\n",
    "        print('hidden:   ', hidden.get_shape().as_list())\n",
    "        \n",
    "        output = tf.nn.relu(tf.matmul(hidden, output_W) + output_b)\n",
    "        print('output:   ', output.get_shape().as_list(),'\\n\\n')\n",
    "        return output\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Minibatch\t Minibatch\t Validation\n",
      "Step\t Loss\t\t Accuracy\t Accuracy\n",
      "0\t 0.725892\t 54.3%\t\t 49.4%\t\n",
      "50\t 0.702979\t 48.4%\t\t 51.7%\t\n",
      "100\t 0.705922\t 49.6%\t\t 53.3%\t\n",
      "150\t 0.687450\t 55.1%\t\t 55.2%\t\n",
      "200\t 0.680434\t 58.2%\t\t 55.9%\t\n",
      "250\t 0.668010\t 60.2%\t\t 51.9%\t\n",
      "300\t 0.665098\t 59.4%\t\t 59.0%\t\n",
      "350\t 0.655646\t 62.1%\t\t 58.4%\t\n",
      "400\t 0.749678\t 46.9%\t\t 51.8%\t\n",
      "450\t 0.621250\t 68.8%\t\t 60.7%\t\n",
      "500\t 0.611697\t 71.9%\t\t 63.7%\t\n",
      "550\t 0.680555\t 56.2%\t\t 60.9%\t\n",
      "600\t 0.641611\t 62.1%\t\t 65.4%\t\n",
      "650\t 0.610430\t 68.0%\t\t 66.7%\t\n",
      "700\t 0.607095\t 67.2%\t\t 62.7%\t\n",
      "750\t 0.652847\t 59.8%\t\t 59.3%\t\n",
      "800\t 0.597264\t 65.6%\t\t 66.1%\t\n",
      "850\t 0.571627\t 73.8%\t\t 67.0%\t\n",
      "900\t 0.610575\t 63.3%\t\t 70.1%\t\n",
      "950\t 0.586581\t 70.7%\t\t 70.6%\t\n",
      "1000\t 0.560463\t 71.5%\t\t 70.6%\t\n",
      "*** TEST ACCURACY: 71.8% ***\n"
     ]
    }
   ],
   "source": [
    "def run_session(num_steps = 1000):\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('\\t',     'Minibatch\\t', 'Minibatch\\t',  'Validation')\n",
    "        print('Step\\t', 'Loss\\t\\t',      'Accuracy\\t',  'Accuracy')\n",
    "        for step in range(num_steps+1):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 50 == 0):\n",
    "                print('%d\\t %f\\t %.1f%%\\t\\t %.1f%%\\t' % (\n",
    "                    step,\n",
    "                    l,\n",
    "                    accuracy(predictions, batch_labels),\n",
    "                    accuracy(valid_prediction.eval(), valid_labels)\n",
    "                ))\n",
    "        print('*** TEST ACCURACY: %.1f%% ***' % accuracy(test_prediction.eval(), test_labels))\n",
    "        prediction_ndarray = test_prediction.eval()\n",
    "\n",
    "run_session(num_steps = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Minibatch\t Minibatch\t Validation\n",
      "Step\t Loss\t\t Accuracy\t Accuracy\n",
      "0\t 0.704212\t 46.5%\t\t 50.3%\t\n",
      "50\t 0.711170\t 50.0%\t\t 51.7%\t\n",
      "100\t 0.702906\t 50.0%\t\t 53.5%\t\n",
      "150\t 0.681891\t 53.9%\t\t 55.6%\t\n",
      "200\t 0.694169\t 52.7%\t\t 57.0%\t\n",
      "250\t 0.661604\t 60.5%\t\t 52.5%\t\n",
      "300\t 0.652424\t 63.7%\t\t 60.2%\t\n",
      "350\t 0.648504\t 60.9%\t\t 58.5%\t\n",
      "400\t 0.758930\t 47.7%\t\t 51.2%\t\n",
      "450\t 0.606714\t 69.9%\t\t 61.2%\t\n",
      "500\t 0.602666\t 68.0%\t\t 64.5%\t\n",
      "550\t 0.666663\t 55.1%\t\t 63.6%\t\n",
      "600\t 0.608256\t 68.4%\t\t 66.3%\t\n",
      "650\t 0.617038\t 64.8%\t\t 67.7%\t\n",
      "700\t 0.613045\t 63.3%\t\t 63.1%\t\n",
      "750\t 0.659635\t 58.6%\t\t 59.8%\t\n",
      "800\t 0.584620\t 68.0%\t\t 67.6%\t\n",
      "850\t 0.565616\t 70.7%\t\t 67.7%\t\n",
      "900\t 0.610997\t 65.2%\t\t 70.2%\t\n",
      "950\t 0.587207\t 72.3%\t\t 70.8%\t\n",
      "1000\t 0.595937\t 65.6%\t\t 68.7%\t\n",
      "1050\t 0.650272\t 62.9%\t\t 63.7%\t\n",
      "1100\t 0.549253\t 73.0%\t\t 71.5%\t\n",
      "1150\t 0.647494\t 62.5%\t\t 65.4%\t\n",
      "1200\t 0.531154\t 73.0%\t\t 71.1%\t\n",
      "1250\t 0.545054\t 73.0%\t\t 70.8%\t\n",
      "1300\t 0.604557\t 70.7%\t\t 67.9%\t\n",
      "1350\t 0.572152\t 69.1%\t\t 70.9%\t\n",
      "1400\t 0.535430\t 70.7%\t\t 68.5%\t\n",
      "1450\t 0.517623\t 73.8%\t\t 71.8%\t\n",
      "1500\t 0.623158\t 64.8%\t\t 70.5%\t\n",
      "1550\t 0.529861\t 73.0%\t\t 73.4%\t\n",
      "1600\t 0.554052\t 71.1%\t\t 72.7%\t\n",
      "1650\t 0.602054\t 64.8%\t\t 66.2%\t\n",
      "1700\t 0.559003\t 70.3%\t\t 72.6%\t\n",
      "1750\t 0.520869\t 76.2%\t\t 72.5%\t\n",
      "1800\t 0.537416\t 74.2%\t\t 72.5%\t\n",
      "1850\t 0.587606\t 64.8%\t\t 68.4%\t\n",
      "1900\t 0.552073\t 69.5%\t\t 72.7%\t\n",
      "1950\t 0.624915\t 61.7%\t\t 65.2%\t\n",
      "2000\t 0.531805\t 71.1%\t\t 71.7%\t\n",
      "*** TEST ACCURACY: 72.9% ***\n"
     ]
    }
   ],
   "source": [
    "run_session(num_steps = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.36532587  0.63467413]\n",
      " [ 0.36353308  0.63646686]\n",
      " [ 0.76541942  0.23458053]\n",
      " ..., \n",
      " [ 0.29438186  0.70561814]\n",
      " [ 0.79588997  0.20411004]\n",
      " [ 0.50122339  0.49877667]]\n",
      "results:  [1 1 0 ..., 1 0 0]\n",
      "Prediction result saved to ./Prediction_19383.data\n"
     ]
    }
   ],
   "source": [
    "## Export file\n",
    "DIST = './Prediction_19383.data'\n",
    "\n",
    "\n",
    "print(prediction_ndarray)\n",
    "results = np.argmax(prediction_ndarray, axis=1)\n",
    "print('results: ', results)\n",
    "\n",
    "with open(DIST, 'w') as f:\n",
    "    for r in results:\n",
    "        f.write(str(r)+'\\n')        \n",
    "        f.close\n",
    "    print('Prediction result saved to',DIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
