{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Protein-DNA binding prediction (CTCF)\n",
    "=============\n",
    "\n",
    "Goal: implement DeepBing with TensorFlow\n",
    "------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "***\n",
    "## DeepBind:\n",
    "\n",
    "> <img src=\"http://www.nature.com/nbt/journal/v33/n8/images/nbt.3300-F2.jpg\" width=\"80%\">\n",
    "> Source: http://www.nature.com/nbt/journal/v33/n8/images/nbt.3300-F2.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train.pickle:  (77531, 121, 4) (77531, 2)\n",
      "Loaded test.pickle:  (19383, 121, 4)\n",
      "\n",
      "Training set:\t range(0, 62024) (62024, 121, 4) (62024, 2)\n",
      "Validation set:\t range(62025, 77531) (15506, 121, 4) (15506, 2)\n",
      "Test set:\t range(0, 19383) (19383, 121, 4) (19383, 2)\n"
     ]
    }
   ],
   "source": [
    "K_FOLD = [4,1] ## A 5 fold\n",
    "TRAIN_PICKLE = '../../../train.pickle'\n",
    "TEST_PICKLE  = '../../../test.pickle'\n",
    "\n",
    "with open(TRAIN_PICKLE, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    seq  = save['seq']\n",
    "    label= save['label']\n",
    "    len1 = seq.shape[0]\n",
    "    del save\n",
    "    print('Loaded train.pickle: ',seq.shape,label.shape)\n",
    "\n",
    "with open(TEST_PICKLE, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    test_seq = save['seq']\n",
    "    test_label = save['label']\n",
    "    len2 = test_seq.shape[0]\n",
    "    del save\n",
    "    print('Loaded test.pickle: ',test_seq.shape)\n",
    "\n",
    "\n",
    "_ = int((len1-len1%(sum(K_FOLD)))/(sum(K_FOLD))) ## 15506\n",
    "r_train = range(0,4*_)\n",
    "r_valid = range(4*_+1,len1)\n",
    "r_test  = range(0,len2)\n",
    "\n",
    "train_dataset, train_labels = seq[r_train,], label[r_train,]  \n",
    "valid_dataset, valid_labels = seq[r_valid,], label[r_valid,]\n",
    "test_dataset, test_labels   = seq[r_test,] , label[r_test,]  \n",
    "\n",
    "print('\\nTraining set:\\t',     r_train,  train_dataset.shape,  train_labels.shape)\n",
    "print('Validation set:\\t', r_valid,  valid_dataset.shape,  valid_labels.shape)\n",
    "print('Test set:\\t',         r_test,   test_dataset.shape,   test_labels.shape)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (62024, 121, 4, 1) (62024, 2)\n",
      "Validation set (15506, 121, 4, 1) (15506, 2)\n",
      "Test set (19383, 121, 4, 1) (19383, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_channels = 1\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, 121, 4, num_channels)).astype(np.float32)\n",
    "#   labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepBind CNN Model\n",
    "> <img src=\"http://www.nature.com/nbt/journal/v33/n8/images/nbt.3300-SF1.jpg\" width=\"90%\">\n",
    ">... Shown is an example with __batch_size=5, motif_len=6, num_motifs=4, num_models=3__. Sequences are padded with ‘N’s so that the motif scan operation can find detections at both extremities. Yellow cells represent the reverse complement of the input located above; both strands are fed to the model, and the strand with the maximum score is used for the output prediction (the max strand stage). The output dimension of the pool stage, depicted as num_motifs (*), depends on whether “max” or “max and avg” pooling was used.\n",
    "> \n",
    "> Image source: http://www.nature.com/nbt/journal/v33/n8/fig_tab/nbt.3300_SF1.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           [batch, height, width, channel]\n",
      "data:      [5, 121, 4, 1]\n",
      "conv:      [5, 121, 4, 16]\n",
      "relu:      [5, 121, 4, 16]\n",
      "pooling:   [5, 60, 2, 16]\n",
      "reshape:   [5, 1920]\n",
      "hidden:    [5, 32]\n",
      "output:    [5, 2] \n",
      "\n",
      "\n",
      "           [batch, height, width, channel]\n",
      "data:      [15506, 121, 4, 1]\n",
      "conv:      [15506, 121, 4, 16]\n",
      "relu:      [15506, 121, 4, 16]\n",
      "pooling:   [15506, 60, 2, 16]\n",
      "reshape:   [15506, 1920]\n",
      "hidden:    [15506, 32]\n",
      "output:    [15506, 2] \n",
      "\n",
      "\n",
      "           [batch, height, width, channel]\n",
      "data:      [19383, 121, 4, 1]\n",
      "conv:      [19383, 121, 4, 16]\n",
      "relu:      [19383, 121, 4, 16]\n",
      "pooling:   [19383, 60, 2, 16]\n",
      "reshape:   [19383, 1920]\n",
      "hidden:    [19383, 32]\n",
      "output:    [19383, 2] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_size   = [121,4]  ## 101bps, plus 10bps frenking on both end\n",
    "num_labels   = 2        ## bind or not (1 or 0)\n",
    "batch_size   = 5        ## TODO: try with double strand input!\n",
    "filter_size  = [11,3]   ## Motif detector length = 11 (about 1.5 times of expected motif length)\n",
    "depth        = 16       ## Number of motif detector (num_motif) = 16\n",
    "num_hidden   = 32       ## 32 ReLU units of no hidden layer at all\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size[0], image_size[1], num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # conv filter, shape=(11,3)\n",
    "    filter_W = tf.Variable(tf.truncated_normal([11, 3, num_channels, depth], stddev=0.1)) ## difference ?\n",
    "    filter_b = tf.Variable(tf.zeros([depth]))\n",
    "\n",
    "    # NN layer, shape=()\n",
    "    hidden_W = tf.Variable(tf.truncated_normal([1920, 32], stddev=0.1))\n",
    "    hidden_b = tf.Variable(tf.constant(1.0, shape=[32]))\n",
    "    \n",
    "    # output layer, shape=()\n",
    "    output_W = tf.Variable(tf.truncated_normal([32, 2], stddev=0.1))\n",
    "    output_b = tf.Variable(tf.constant(1.0, shape=[2]))\n",
    " \n",
    "\n",
    "    ## Model.\n",
    "    def model(data):\n",
    "        print('           [batch, height, width, channel]') \n",
    "        print('data:     ', data.get_shape().as_list())\n",
    "        \n",
    "        # Convolution: (121,4,1) ---- 3x6 filter ---> (121,4,4)\n",
    "        conv = tf.nn.conv2d(data, filter_W, [1, 1, 1, 1], padding='SAME')\n",
    "        print('conv:     ', conv.get_shape().as_list())\n",
    "        \n",
    "        relu = tf.nn.relu(conv + filter_b)\n",
    "        print('relu:     ', relu.get_shape().as_list())\n",
    "\n",
    "        pool = tf.nn.max_pool(relu, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "        print('pooling:  ', pool.get_shape().as_list())\n",
    "        \n",
    "        shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        print('reshape:  ', reshape.get_shape().as_list())\n",
    "        \n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, hidden_W) + hidden_b)\n",
    "        print('hidden:   ', hidden.get_shape().as_list())\n",
    "        \n",
    "        output = tf.nn.relu(tf.matmul(hidden, output_W) + output_b)\n",
    "        print('output:   ', output.get_shape().as_list(),'\\n\\n')\n",
    "        return output\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "    \n",
    "#     test_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "#     valid_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Minibatch\t Minibatch\t Validation\n",
      "Step\t Loss\t\t Accuracy\t Accuracy\n",
      "0\t 0.780429\t 40.0%\t\t 50.0%\t\n",
      "50\t 0.693147\t 20.0%\t\t 50.0%\t\n",
      "100\t 0.693147\t 80.0%\t\t 50.0%\t\n",
      "150\t 0.693147\t 100.0%\t\t 50.0%\t\n",
      "200\t 0.693147\t 80.0%\t\t 50.0%\t\n",
      "250\t 0.693147\t 0.0%\t\t 50.0%\t\n",
      "300\t 0.693147\t 100.0%\t\t 50.0%\t\n",
      "350\t 0.693147\t 40.0%\t\t 50.0%\t\n",
      "400\t 0.693147\t 80.0%\t\t 50.0%\t\n",
      "450\t 0.693147\t 40.0%\t\t 50.0%\t\n",
      "500\t 0.693147\t 80.0%\t\t 50.0%\t\n",
      "550\t 0.693147\t 60.0%\t\t 50.0%\t\n",
      "600\t 0.693147\t 20.0%\t\t 50.0%\t\n",
      "650\t 0.693147\t 40.0%\t\t 50.0%\t\n",
      "700\t 0.693147\t 60.0%\t\t 50.0%\t\n",
      "750\t 0.693147\t 60.0%\t\t 50.0%\t\n",
      "800\t 0.693147\t 60.0%\t\t 50.0%\t\n",
      "850\t 0.693147\t 40.0%\t\t 50.0%\t\n",
      "900\t 0.693147\t 20.0%\t\t 50.0%\t\n",
      "950\t 0.693147\t 60.0%\t\t 50.0%\t\n",
      "1000\t 0.693147\t 20.0%\t\t 50.0%\t\n",
      "*** TEST ACCURACY: 50.7% ***\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('\\t',     'Minibatch\\t', 'Minibatch\\t',  'Validation')\n",
    "    print('Step\\t', 'Loss\\t\\t',      'Accuracy\\t',  'Accuracy')\n",
    "    for step in range(num_steps+1):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('%d\\t %f\\t %.1f%%\\t\\t %.1f%%\\t' % (\n",
    "                step,\n",
    "                l,\n",
    "                accuracy(predictions, batch_labels),\n",
    "                accuracy(valid_prediction.eval(), valid_labels)\n",
    "            ))\n",
    "    print('*** TEST ACCURACY: %.1f%% ***' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
